# TensorStrainers

Tensor-Brick is a Chisel-based generator of low-rank convolution accelerators. It includes a library of tensor-based linear-algebra operators, 
streaming memory interfaces, and tensor-based buffers. This leads to a composable architecture specification that optimizes across multiple layers in a DNN.
Tensor-Brick is a framework for synthesizing and generating the microarchitecture of cross-layer pipelines.
The Tensor-Bricks generator is written in Chisel and it standardizes the interfaces between components and creates an environment where
i) we can generate pipeline for different low-rank convolution accelerators by simply changing the connectivity of the components, 
and ii) affect changes in the microarchitecture (e.g., number of tiles, introduce fine-grain buffers) based on the parameters passed to the generator.


The core of Tensor-Brick is a hardware library of building blocks (*bricks*) that provides parameterized design for:
i) creating pipelined units that execute arbitrary acyclical dataflow graphs of *linear algebra operators*.
The unit of operand in all these operators is a tensor (rank and shape can be set by designer),
ii) *multi-stream interfaces* that decouple the compute unit from the underlying tensor storage and overlaps data movement with computation.
iii) *layer-buffers* that capture data-reuse across layers and simplify the process of exchanging data across layers.

## System Architecture

Tensor-Brick creates an accelerator that can be easily deployed on SoC-FPGAs like Altera Cyclone-V or Xilinx Zynq boards. The structure of the generated accelerator is shown below. The accelerator is connected to the host processor from one side and to the off-chip memory form the other side using two AXI interfaces. The host processor controls the accelerator and reads its status through a memory-mapped structure. Since the host initiates the transactions, host is the AXI master and the accelerator is AXI client. On the other hand, the accelerator handles the data movement from/to off-chip memory, then it is equipped with an AXI master to connect to the AXI client interface of off-chip memory. 

The parameters of the AXI interfaces can be set in file: *src/main/scala/shell/AXI.scala*. For more information about the SoC interfaces, please read the following document:

* [Accelerator Interface](./doc/Soc-Interface.md)


![Accelerator](https://www.dropbox.com/s/7s2ihdq1r7evnig/Tensor-Brick%20Arch..png?raw=1)


The class hierarchy of the DNN accelerator generated by tensor-brick is shown below. A DNNAccel is composed of DNNCore,
host interface and memory interface. DNNCore consists of building blocks or bricks for computation and memory load/store.
Host interface contains an AXI client to connect to the host processor. Memory controller contains an AXI master to 
read/write from/to main memory. The code can be found in this file:

``
src/main/scala/accel/DNNAccel.scala
``



![DNNAccel](https://www.dropbox.com/s/mhjtmzx4d75888r/DNNAccel_hierarchy.png?raw=1)

## Bricks

Tensor-Brick provides the building blocks (bricks) required for convolution layers in DNNs. 
By arranging these bricks in an proper order, we can create a wide range of convolutions, from low-rank convolutions to spatial (standard) convolutions.
This helps us to find the optimum design point where the hardware and memory bandwidth utilization are maximized. 
We have classified these bricks into three main classes: *Layer-Brick (L-Brick), Memory-Brick (M-Brick) and Intermediate-Brick(I-Brick)*.

### L-Brick

**L-Brick** is a heterogeneous systolic datapath that can support linear algebra operations on user-defined tensor types.
L-Brick is composed of linear operations such as element-wise Dot product, element-wise Reduce, GEMM and GEMV.
 As shown in the figure below, the main linear operations in a convolution brick are element-wise Dot product and Reduce. 
 All the operations are implemented in a systolic style. In this example, the size of convolutions, number of convolutions and the shape of operands are 
 parameterized. Using these parameters, user can create any arbitrary convolution component. For instance, this example targets depth-wise convolutions where 
 several 3x3 filters are convolved by the input images. Similarly, user can create a non-square convolution or a point-wise convolution.

![L-Brick](https://www.dropbox.com/s/rzwjty6evsgvs7j/L-Brick.png?raw=1)


### M-Brick

**M-Brick** is 
 responsible for taking the wide range of data storage formats and converting them into a universal stream of tensors for the L-Bricks's datapath.
 The main challenge that L-Brick deals with is the mismatch between the width of DRAM burst interfaces and the L-Brick compute units.
 For instance, a single DRAM burst could potentially feed up to 64bits (8 bytes) to the accelerator. 
 However, the compute operator's requirements depth-wise convolution (3x3 = 9 elements) or point-wise (1x3 = 3 elements) are 
 mismatched against the DRAM burst width. M-Brick uses a set of buffers (Tensorfile) to hold data from the DRAM interface prior to being streamed into the L-Brick.
 The tensorized buffers is a bank of single-ported (1R-1W) RAMs that supply a subset of the tensor required by the L-Brick. 

 The rows of the tensor are interleaved across the different banks. Since only one memory port exists, an arbiter is needed to arbitrate the memory read/write requests 
 in a round-robin manner and connect the selected bank to the memory port.
 The M-Brick supplies a tensor to the L-Brick operators by simultaneously reading multiple rows from each of the banks.
 
![M-Brick](https://www.dropbox.com/s/kxjwenrt3s08r0b/M-Brick.png?raw=1)

### I-Brick

In cross-layer implementation, we need to connect multiple L-Bricks together to make the pipeline. Connecting two L-Bricks with different input shapes is not trivial.
To keep the L-Brick design extensible and composable, we separate the connection wiring and handshaking from the operatorâ€™s logic.
Each L-Brick can independently choose the appropriate dataflow and hyper parameters. 
The I-Bricks between layers are used to buffer and transform the multi-dimensional tensor data as it passes through layers. 
They contain minimal logic and are largely composed of routing wires and short shift buffers for staging the the tensor streams. 
The I-Brick microarchitecture uses the information on the dataflow of the layer stages and other hyper parameters to take any tensor format and
 output the required tensor format. This eases the connection between L-Bricks.
 
From microarchitecture perspective, I-Brick is a Multiple-Input Multiple-Output (MIMO) queue which is flexible and parameterized. Queues (FIFOs) play a key role
when the speed of producer component (which writes to the queue) is not same as the speed of consumer component (which reads from the queue). When both producer and consumer 
writes/reads one element at the time (even with different rates), a single-input single-output queue is sufficient to make the connection between them. 
However, if the size of producer's data is different from the consumer's data, then a single-input single-output can not handle the communication.
In this case, we need a multiple-input multiple-output queue that can different input-output port size. For instance, the producer components produces data chunks of size
8 bytes, while the consumer needs chunks of size 3 bytes. As shown in the figure below, a MIMO queue can accept N elements as the input (per write) and delivers M elements
as the output (per read). 

Despite of this advantage of MIMO queue, it plays a key role in transforming the shape of data (tensors) from an L-Brick to another L-Brick. 


#### Packing

As shown in the figure below, an I-Brick (MIMO queue) is able to pack multiple streams coming from different components and feed to the down-stream component. 
For instance, in a standard 2D convolution with 3x3 filter, three streams from three L-Bricks have to be packed and sent to the corresponding L-Brick to perform the convolution.
As shown in the figure, an I-Brick packs three streams and sends to the L-Brick. Note that the speed and size of input data may be different from output data. The up-stream
L-Bricks (producers) write 3xN elements per cycle, while the consumer reads M elements per cycle.

#### Merging

In addition to packing, I-Brick helps us to reformat the data when sending from one stage to another one. For example, as shown in the figure below, 
the first output of up-stream L-Bricks should be packed and sent to the first input of down-stream L-Brick. Similarly, the second output of up-stream L-Bricks
should be packed and sent to the second output of down-stream L-Brick. This can be easily done by two I-Bricks. Note that this transformation is exploited frequently 
in low-rank convolutions. In particular, when a depth-wise layer is connected to a point-wise layer, the tensor format should be changed from  CWH to HWC. Using the I-Brick
component, we can connect the L-Bricks and create a pipelined structure to perform cross-layer convolutions. 

![I-Brick](https://www.dropbox.com/s/unse5z7g5dut4wp/I-Brick.png?raw=1)
 
 
 We believe that using these bricks, we can generate a wise range of architectures needed in DNN accelerators, 
 particularly, low-rank convolutions. Tensor bricks help us to create a pipelines architecture to maximize the memory 
 bandwidth and resource utilization. Besides, optimizing the pipeline helps us to keep contiguous streaming data 
 from/to off-chip memory. In summary, we can list the advantages of tensor-bricks as follows:
 
  - Flexibility to create arbitrary pipeline graphs.
  - Flexibility in permitting layers in the pipeline to be internally tiled and parallelized.
  - Flexibility in PE definition (e.g., arbitrary dataflow to capture spatial reuse, temporal reuse), 
  - Customizable numeric types (binary point or fixed) 
  - Automated management of data movement in the pipeline

 In what follows, we show how to create a point-wise block for convolutions using tensor bricks. This can be generalized
 to create any arbitrary, pipelined architecture. We have implemented four building blocks which are widely used in 
 low-rank convolutions: Point-wise (PW), Depth-wise (DW), 
 Point-wise_Depth-wise (DP) and Point-wise_Depth-wise_Point-wise (PDP). All of them are composed of M-Bricks, I-Bricks 
 and L-Bricks which are parameterized and can be configured to perform any arbitrary batch size of a low-rank convolution.
 
  
 ## Create a Point-wise Block in Tensor-Brick
 
 The microarchitecture of a point-wise block is shown in the figure below. Here, we explain how to create a 
 point-wise accelerator using the designed bricks. User only needs to declare the basic parameters which are the 
 size of the batch. In a point-wise layer, we have a set of 1x1 filters. The depth of each filter equals to the depth of 
 the input image. We need four parameters to create the accelerator: height, width and depth of the batch and number of 
 filters:
 
 1. Hx: Number of rows
 
 2. Wx: Number of columns
 
 3. Cx: depth of the batch
 
 4. Fx: Number of 1x1 filters
 
 These parameters are defined in the file below. However, they can be set directly from the Makefile while making the project and
 generating the Verilog file.
 
``
src/main/scala/accel/DNNCore.scala
``
 
```scala
  val Hx = 5  //  Number of Rows
  val Wx = 7
  val Fx = 2
  val Cx = 6
```
 
 
 ![PW_Block](https://www.dropbox.com/s/wl4fk3k16z7eu5l/PW.png?raw=1)
 
 
 ## Customizing the types
 
 In the file DNNCore.scala, after initializing the parameters (batch size and filter size), we should specify the 
 shape and type of the operands. As shown below, the first line is defining the type of operands, which is floating point
 in this example. We can use both floating point (single or double precision) and fixed point with any arbitrary 
 integer and fraction bitwidth. 
 
 ```scala
val S = new FType(8, 24)

  val memShape = new FPvecN(16, S, 0)
  val CxShape = new FPvecN(Cx, S, 0)
```

In this example, since we want to convolve the 1x1 filters, we set the shape to a vector of FP values.
The size of filters is Cx, which is same as the depth of batch. Then, CxShape is set to a FP vector of size Cx and FP
values.

To change the type of operands to fixed-point, we can use the following line:

```scala
val CxShape2 = new FXvecN(Cx, fraction = 10, 0)
```

where the fraction is 10 bits and the integer part is 22 bits. Note that the bitwidth of values can be set in
the config file (see **xlen** in the file below). In this case, the bitwidth is 32 bits.

``
src/main/scala/config/config.scala
``

In the line below, the PW_block is instantiated. As shown, the shapes and parameters are fed to the PW_Block class.
We will discuss about the building blocks of PW_Block in details.

```scala
  val conv = Module(new PW_Block(Hx, Fx, Cb, "intWgtPW1", "inp")(memShape)(CxShape))
```
 
 As shown in the figure, we need three types of bricks: M-Brick for reading/writing from/to memory, I-Brick for
 transforming the shapes and L-Brick for computation. 
 
 
 
 
 The chisel implementation of point-wise block can be found in 
 this file:
 
 ``
 src/main/scala/dnn_layers/PW_Block.scala
 ``
 
 ## Creating tiled memory interfaces in PW_Block (M-Bricks)
 
 In the point-wise block, three M-Bricks are needed: one for reading the inputs, one for reading the weights and
 one brick for writing the outputs. These bricks are defined in lines 72, 98 and 81, respectively:
 
 ```scala
 #72: val M_Brick_in =  Module(new inDMA_act_HWC(Hx, 1, memTensorType)(memShape))
 ```
 
 ```scala
 #98: val M_Brick_wgt = Module(new inDMA_wgt(wgtTFDepth = 20, bufSize = 100, wgtType, memTensorType)(CxShape)) 
 ```
 
 ```scala
 #81: val M_Brick_out = for (i <- 0 until Fx) yield {
       val outDMA = Module(new outDMA_act(Hx, bufSize = 20, memTensorType))
       outDMA
     }
 ```
 
 As shown in the figure, Hx is the number of rows in the batch. In M_Brick_in which reads the input from memory, we need
 Hx memory banks, each of them is assigned to the corresponding row in the input batch. Since the tensor is stored in 
 memory in HWC format, each bank of M_Brick_in should be of size Wx * Cx elements.
 
 We need another M_Brick to read and hold the weights. Since the weights are of size Cx * 1, the shape of weights should 
 be passed to this M-Brick. 
 
 Given Hx rows of input and Fx filters, the output of this batch would be Hx*Fx rows, each of size Wx. Then, we need
 Fx M-Bricks, each of size Hx for writing the output to the memory.
 
 
 ## Data movement management in PW_Block (I-Bricks)
 
 The width of BRAM banks in M_Brick_in is a multiple of AXI bitwidth, in this example 64 bits. While the L-Brick needs 
 Cx values at a time to do the dot product. Then, we need to change the shape of data when we are connecting the M-brick
 to the L-Brick. Line 72 shows the definition of the L-Brick. It needs the shape of input data which is stores in M-Brick
 from one side, and the shape of weight needed for convolution form the other side.
 
 ```scala
 #72: val I_Brick = Module(new PWShapeTransformer(Hx, Fx, bufSize = 20, memTensorType)(CxShape))
 ```
 
 The I-Brick is a MIMO queue which can transform the shape of input data and delivers it in a different shape in the 
 output side. The depth of its buffer is specified by 'bufSize'. The buffer size should be greater than both input shape
 and output shape. For instance, if the producer (M-Brick in this example) writes 16 values at a time into the buffer, while
 the consumer (L-Brick) reads 9 values at a time, the depth of the buffer should be at least 16 elements.
 
 ## Dataflow definition in PW_Block (L-Bricks)
 
 In this example, the input batch has Hx rows that should be convolved by Fx filters. Then, we need Hx * Fx convolution
 nodes to perform the computations. The instantiation of the L-Bricks is shown in line 74:
 
 ```scala
 #72: val L_Brick = for (i <- 0 until Fx) yield {
       val mac1d = Module(new Mac1D(Hx, ChBatch, wgtType)(CxShape))
       mac1d
   }
 ```
 
 
 Each CONV is able to perform a convolution of size Cx*1. Each CONV node is equipped 
 with a Dot product node and a reduce node to calculate the sum of the elements. On the other side, the L-Bricks are
 connected to the M_Brick_wgt to read the corresponding weights. First, L-Bricks read their weights from the M_Brick_wgt. Once
 the weights have been stored in the L-Bricks, they start reading the input values from the M_Brick_in through the I-Brick.
 
 The class hierarchy of a Mac node (in this case, Mac node contains a Dot node and a Reduce node) is shown in the figure 
 below. Each Dot node is composed of a 1D array of PEs, each of them can perform a mac operation. Also, the type of 
 operation is extracted from the input shape of the Mac Node; in this example, since the input shape is Floating Point,
 the generated PEs are using FP units. 
 
 
 ![MacNode_hierarchy](https://www.dropbox.com/s/xujxczbx4v6izic/MacNode_hierarchy.png?raw=1)

 
 
## Run
```
make TOP=TestAccel2

```

## Get F1Shell
```
make verilog TOP=TestAccelAWS
```

## To change configuration 
* `DNNCorePW.scala` change `Hx,Wx,Cx,Cb,Fx`, where `C=Cx*Cb` and Cx ~= 1.
* `VME.scala` 
```
  val nReadClients: Int = Hx+1
  val nWriteClients: Int = Fx*Hx
```

## Debugging

* If you see the following error: 
```
# in red color
>TOP__TestAccel2__DOT__vta_shell__DOT__core__DOT__conv__DOT__mac1D_0__DOT__mac_2__reduceNode__DOT__FU__DOT__NCycle_Reduction__DOT__PEs_2
```
Go to 
```
rm build/verilator/*
```
and then run make again. 
